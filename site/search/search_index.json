{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Images/ cropped.jpg css/ style.css","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Images/ cropped.jpg css/ style.css","title":"Project layout"},{"location":"about/","text":"This is about our project with ITU. Who are we? Tova Perlman Utku Can Ozturk Jonathan Cook Jacob Beck Project Mentor: Daniel Townsend Technical Mentor: Robert Hager Who is ITU? Who is UNICEF? What is Project Giga?","title":"About the Team"},{"location":"conc/","text":"Conclusion Discussion Limitations Next Steps Additional Methods Multiple additional methods could be applied to our existing models and analyses: Full scoring on Brazil Given our existing champion model, all schools outside the featured enumeration areas can now be scored with an estimated level of connectivtiy. This could be done for a sample of schools as a sanity check or for entire Brazil, which would yield a similar map like the Giga Initiative connectivtiy map. Statistical robustness checks Especially two steps could be reasonable additions that take the geographic nature of the data into account: Estimating a Geographically Weighted Regression (GWR) and performing spatial cross-validation on the models. Experiment with featured data The manner in which we used some of the features could be varied. First of all, it could be fruitful to experiment with Facebook data as an alternative target variable. If model performance still remains on an acceptable level, this would render Microdata obsolete and would enable a global extension of our project. Furthermore, pulling OSM school locations for Brazil and applying the model to this sample could yield insights on the representativity of OSM data. Lastly, buffer zones around schools are currently constant in diameter, but could be varied according to specific variables (e.g. population data). Extension on other countries The existing model can be applied to every country, with OSM school location data available. However,model evaluation or model training for other countries requires two more components: Microdata (on household, individual or enumeration area level) and the respective shapefiles/geolocation of the enumeration area. Our analyses have indicated, that analyzing larger geographical aggregates like provinces diminishes model performance and interpretability. An enumeration area is assumed to be a largely homogeneous area, whereas the variation of connectivtiy within e.g. a federal state should be much higher. Further model training With more countries being included to the connectivity analysis, more options of model combinations open up. Given the requirements are met, an individual model can be trained for each country and evaluated separately. In addition, joint models with multiple countries could result in a high predictive power on a global level and increase model robustness. Comparing the performances of combined and single-country models can give insights on the impact of country specific differences. Furthermore, it might be a reasonable idea to train regional models (e.g. a model for South East Asian countries). Actors that are not able to provide enumeration area level microdata (e.g. due to anonymity restrictions) can obtain our provided scripts and additional content and run a custom model training. If we had another few months, we would have trained a model on Thailand and perhaps one on the Phillipines. Then we would have loved to combine them into one model to predict for a fourth country. As mentioned prior, we do not have national level statistics or information in any of these models since they are trained on one country exclusively. If we were able to combine different models, we could better know if there is some variance explained by regional or national levels of information as opposed to granular scale information.","title":"Conclusion"},{"location":"conc/#conclusion","text":"","title":"Conclusion"},{"location":"conc/#discussion","text":"","title":"Discussion"},{"location":"conc/#limitations","text":"","title":"Limitations"},{"location":"conc/#next-steps","text":"","title":"Next Steps"},{"location":"conc/#additional-methods","text":"Multiple additional methods could be applied to our existing models and analyses: Full scoring on Brazil Given our existing champion model, all schools outside the featured enumeration areas can now be scored with an estimated level of connectivtiy. This could be done for a sample of schools as a sanity check or for entire Brazil, which would yield a similar map like the Giga Initiative connectivtiy map. Statistical robustness checks Especially two steps could be reasonable additions that take the geographic nature of the data into account: Estimating a Geographically Weighted Regression (GWR) and performing spatial cross-validation on the models. Experiment with featured data The manner in which we used some of the features could be varied. First of all, it could be fruitful to experiment with Facebook data as an alternative target variable. If model performance still remains on an acceptable level, this would render Microdata obsolete and would enable a global extension of our project. Furthermore, pulling OSM school locations for Brazil and applying the model to this sample could yield insights on the representativity of OSM data. Lastly, buffer zones around schools are currently constant in diameter, but could be varied according to specific variables (e.g. population data).","title":"Additional Methods"},{"location":"conc/#extension-on-other-countries","text":"The existing model can be applied to every country, with OSM school location data available. However,model evaluation or model training for other countries requires two more components: Microdata (on household, individual or enumeration area level) and the respective shapefiles/geolocation of the enumeration area. Our analyses have indicated, that analyzing larger geographical aggregates like provinces diminishes model performance and interpretability. An enumeration area is assumed to be a largely homogeneous area, whereas the variation of connectivtiy within e.g. a federal state should be much higher.","title":"Extension on other countries"},{"location":"conc/#further-model-training","text":"With more countries being included to the connectivity analysis, more options of model combinations open up. Given the requirements are met, an individual model can be trained for each country and evaluated separately. In addition, joint models with multiple countries could result in a high predictive power on a global level and increase model robustness. Comparing the performances of combined and single-country models can give insights on the impact of country specific differences. Furthermore, it might be a reasonable idea to train regional models (e.g. a model for South East Asian countries). Actors that are not able to provide enumeration area level microdata (e.g. due to anonymity restrictions) can obtain our provided scripts and additional content and run a custom model training. If we had another few months, we would have trained a model on Thailand and perhaps one on the Phillipines. Then we would have loved to combine them into one model to predict for a fourth country. As mentioned prior, we do not have national level statistics or information in any of these models since they are trained on one country exclusively. If we were able to combine different models, we could better know if there is some variance explained by regional or national levels of information as opposed to granular scale information.","title":"Further model training"},{"location":"configs/","text":"Configurations We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. '../../../files/' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') COUNTRY - Country code for current use-case, e.g. 'tha' FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown.","title":"Configurations"},{"location":"configs/#configurations","text":"We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. '../../../files/' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') COUNTRY - Country code for current use-case, e.g. 'tha' FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown.","title":"Configurations"},{"location":"data_dictionaries/","text":"Data Dictionaries","title":"Data Dictionaries"},{"location":"data_dictionaries/#data-dictionaries","text":"","title":"Data Dictionaries"},{"location":"data_processing/","text":"Instructions to Create Model-Ready Dataset To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use-case configurations in configs.py. Details of the configurable variables and their expected assignments can be found in the 'configurations' section of the documentation. Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory.","title":"Instructions to Create Model-Ready Dataset"},{"location":"data_processing/#instructions-to-create-model-ready-dataset","text":"To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use-case configurations in configs.py. Details of the configurable variables and their expected assignments can be found in the 'configurations' section of the documentation. Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory.","title":"Instructions to Create Model-Ready Dataset"},{"location":"datagat/","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Data Gathering Utku did a lot of work on this. Yay Utku!! Internal Data Surveys from ITU for Brazil and Thailand The target variable for our modeling was the proportion of a population around a particular school that was connected to the internet. It therefore ranged from 0-1, with 0 being zero percent connected and 100 being 100% connected to the internet. We chose to measure this on a school level as one of our objectives, through working with UNICEF, was to detect schools that could be connected to the internet and further serve the community they are located. Within the Brazil Survey data, we received information on household internet connectivity on an enumeration area level. This presented a slight challenge as the level of granularity of the school data was slightly different from the enumeration data or census tract. Thus, we matched the school points to the enumeration area data. We could not use all the school points as we only had enumeration areas for a specific amount of tracts in Brazil. Thus we had to subset our school points data to around 11,000 points. Once we connected the schools to the enumeration areas we were able to build our training data set. School Points from UNICEF for Brazil We got the school points in lat, long format from UNICEF for Brazil. Unfortunately, we were not able to obtain the school points for Thailand. We thus turned to OpenStreetMap to obtain school points for Thailand. We obtained many school points but filtered them to the schools that we were positive were schools as some were tagged as dance schools or even ATM's. Our school points script is thus specific for obtaining the OSM points. Should we say something about the licensing? External/Open Source Data We now have to gather all the open data that we've used: Open Cell ID School Population Points using OSM Satellite data To see the full code for gathering this data, click here. To gather the satellite data, we used Google Earth Engine for Python API. We gathered three different types of data: Global Human Modification Index, Nighttime Data, Normalized Difference Vegetation Index. Our hope with gathering this data is that it would provide an accurate proxy for households and schools with internet connection. If we knew a school was located in a place with a high average radiance, it might also mean there was high internet connectivity. The beauty of satellite data is that its continous for the entire globe. We initially struggled with learning how to crop the data for all the school points we wanted. Eventually, we set a 5 km buffer zone around each school point in both Brazil and Thailand and obtained specific satellite information that was input as a number into the training dataset. Below please find more information on each of the datasets we used. Global Human Modification Index (String for Image Collection ID is: 'CSP/HM/GlobalHumanModification'): The global Human Modification dataset (gHM) provides a cumulative measure of human modification of terrestrial lands globally at 1 square-kilometer resolution. The gHM values range from 0.0-1.0 and are calculated by estimating the proportion of a given location (pixel) that is modified, the estimated intensity of modification associated with a given type of human modification or \"stressor\". 5 major anthropogenic stressors circa 2016 were mapped using 13 individual datasets: human settlement (population density, built-up areas) agriculture (cropland, livestock) transportation (major, minor, and two-track roads; railroads) mining and energy production electrical infrastructure (power lines, nighttime lights) NOAA Monthly Nighttime images using the VIIRS Satellite (String for Image Collection ID is: \"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\") Monthly average radiance composite images using nighttime data from the Visible Infrared Imaging Radiometer Suite (VIIRS) Day/Night Band (DNB). As these data are composited monthly, there are many areas of the globe where it is impossible to get good quality data coverage for that month. This can be due to cloud cover, especially in the tropical regions, or due to solar illumination, as happens toward the poles in their respective summer months. Therefore it is recommended that users of these data utilize the 'cf_cvg' band and not assume a value of zero in the average radiance image means that no lights were observed. Normalized Difference Vegetation Index Band from the MODIS dataset (String for Image Collection ID is: 'MODIS/006/MOD13A2'): Normalized Difference Vegetation Index or NDVI measures the vegetation or greenness present on the Earth's surface The algorithm for this product chooses the best available pixel value from all the acquisitions from the 16-day period. The criteria used are low clouds, low view angle, and the highest NDVI/EVI value. Descriptions taken from the Goole Earth Engine Data Catalog. Facebook API data Speedtest Data Data Gathering Classes We divide our data gathering methods into two superclasses: one called OpenData, which is a parent class to each of our open-source data gathering classes; and one called Country, which is a parent class to our school and survey data classes. OpenData: classDiagram OpenData < |-- PopulationData OpenData < |-- SpeedtestData OpenData < |-- FacebookData OpenData < |-- OpenCellData class OpenData{ +set_country_geo() } class PopulationData{ +set_pop_data() } class SpeedtestData{ +type +year +quarter +set_speedtest_data() +tile_prep() } class FacebookData{ +locations +access_token +ad_account_id +call_limit +radius +set_fb_data() } class OpenCellData{ +access_token +set_cell_data() +call_prep() } Country: classDiagram Country < |-- School Country < |-- Survey Survey < |-- BRA_Survey Survey < |-- THA_Survey class Country{ +set_country_geometry() } class School{ +buffer +set_school_data() +school_prep() } class BRA_Survey{ +get_area_links() +set_survey_data() } class THA_Survey{ +set_area_data() +set_survey_data() } Data Dictionary Show a table of each of the predictors and what their definitions are: Variable Name Description Data Source avg_d_kbp3 Average Download Speed Speedtest Data avg_u_kbps Average Upload Speed Speedtest Data estimate_dau Facebook Daily Active Users estimate Facebook API estimate_mau Facebook Monthly Active Users estimate Facebook API population Population within a 1 km buffer zone, estimated with ?? Population Data pop_norm Population normalized Population Data mean_ghm Mean Global Human Modification value Global Human Modification Index mean_avg_rad Mean value from the Average Radiance band VIIRS Nighttime DNB mean_cf_cvg Mean value from the cloud free coverage band VIIRS Nighttime DNB slope_year_avg_rad The yearly rate of change between 2019 and 2014 of Average Radiance VIIRS Nighttime DNB change_year_avg_rad The change between the average values of 2019 and 2014 of Average Radiance VIIRS Nighttime DNB slope_year_cf_cvg The yearly rate of change between 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB change_year_cf_cvg The change between the average values of 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB slope_month_avg_rad The monthly rate of change between 2019 and 2014 of the Average Radiance Band VIIRS Nighttime DNB change_month_avg_rad The change between the average of Dec 2019 and Jan 2014 of the Average Radiance Band VIIRS Nighttime DNB slope_month_cf_cvg The monthly rate of change between 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB change_month_cf_cvg The rate of change between the average of Dec 2019 and Jan 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB mean_NDVI The average value of the Vegetation Index MODIS Dataset slope_year_NDVI The yearly rate of change between 2019 and 2014 of the Vegetation Index MODIS Dataset change_year_NDVI The change between 2019 and 2014 of the Vegetation Index MODIS Dataset slope_month_NDVI The monthly rate of change between 2019 and 2014 of the Vegetation Index MODIS Dataset change_month_NDVI The change between the average of May 2019 and May 2014 of the Vegetation Index MODIS Dataset","title":"Data Gathering"},{"location":"datagat/#data-gathering","text":"Utku did a lot of work on this. Yay Utku!!","title":"Data Gathering"},{"location":"datagat/#internal-data","text":"Surveys from ITU for Brazil and Thailand The target variable for our modeling was the proportion of a population around a particular school that was connected to the internet. It therefore ranged from 0-1, with 0 being zero percent connected and 100 being 100% connected to the internet. We chose to measure this on a school level as one of our objectives, through working with UNICEF, was to detect schools that could be connected to the internet and further serve the community they are located. Within the Brazil Survey data, we received information on household internet connectivity on an enumeration area level. This presented a slight challenge as the level of granularity of the school data was slightly different from the enumeration data or census tract. Thus, we matched the school points to the enumeration area data. We could not use all the school points as we only had enumeration areas for a specific amount of tracts in Brazil. Thus we had to subset our school points data to around 11,000 points. Once we connected the schools to the enumeration areas we were able to build our training data set. School Points from UNICEF for Brazil We got the school points in lat, long format from UNICEF for Brazil. Unfortunately, we were not able to obtain the school points for Thailand. We thus turned to OpenStreetMap to obtain school points for Thailand. We obtained many school points but filtered them to the schools that we were positive were schools as some were tagged as dance schools or even ATM's. Our school points script is thus specific for obtaining the OSM points. Should we say something about the licensing?","title":"Internal Data"},{"location":"datagat/#externalopen-source-data","text":"We now have to gather all the open data that we've used: Open Cell ID School Population Points using OSM Satellite data To see the full code for gathering this data, click here. To gather the satellite data, we used Google Earth Engine for Python API. We gathered three different types of data: Global Human Modification Index, Nighttime Data, Normalized Difference Vegetation Index. Our hope with gathering this data is that it would provide an accurate proxy for households and schools with internet connection. If we knew a school was located in a place with a high average radiance, it might also mean there was high internet connectivity. The beauty of satellite data is that its continous for the entire globe. We initially struggled with learning how to crop the data for all the school points we wanted. Eventually, we set a 5 km buffer zone around each school point in both Brazil and Thailand and obtained specific satellite information that was input as a number into the training dataset. Below please find more information on each of the datasets we used. Global Human Modification Index (String for Image Collection ID is: 'CSP/HM/GlobalHumanModification'): The global Human Modification dataset (gHM) provides a cumulative measure of human modification of terrestrial lands globally at 1 square-kilometer resolution. The gHM values range from 0.0-1.0 and are calculated by estimating the proportion of a given location (pixel) that is modified, the estimated intensity of modification associated with a given type of human modification or \"stressor\". 5 major anthropogenic stressors circa 2016 were mapped using 13 individual datasets: human settlement (population density, built-up areas) agriculture (cropland, livestock) transportation (major, minor, and two-track roads; railroads) mining and energy production electrical infrastructure (power lines, nighttime lights) NOAA Monthly Nighttime images using the VIIRS Satellite (String for Image Collection ID is: \"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\") Monthly average radiance composite images using nighttime data from the Visible Infrared Imaging Radiometer Suite (VIIRS) Day/Night Band (DNB). As these data are composited monthly, there are many areas of the globe where it is impossible to get good quality data coverage for that month. This can be due to cloud cover, especially in the tropical regions, or due to solar illumination, as happens toward the poles in their respective summer months. Therefore it is recommended that users of these data utilize the 'cf_cvg' band and not assume a value of zero in the average radiance image means that no lights were observed. Normalized Difference Vegetation Index Band from the MODIS dataset (String for Image Collection ID is: 'MODIS/006/MOD13A2'): Normalized Difference Vegetation Index or NDVI measures the vegetation or greenness present on the Earth's surface The algorithm for this product chooses the best available pixel value from all the acquisitions from the 16-day period. The criteria used are low clouds, low view angle, and the highest NDVI/EVI value. Descriptions taken from the Goole Earth Engine Data Catalog. Facebook API data Speedtest Data","title":"External/Open Source Data"},{"location":"datagat/#data-gathering-classes","text":"We divide our data gathering methods into two superclasses: one called OpenData, which is a parent class to each of our open-source data gathering classes; and one called Country, which is a parent class to our school and survey data classes.","title":"Data Gathering Classes"},{"location":"datagat/#opendata","text":"classDiagram OpenData < |-- PopulationData OpenData < |-- SpeedtestData OpenData < |-- FacebookData OpenData < |-- OpenCellData class OpenData{ +set_country_geo() } class PopulationData{ +set_pop_data() } class SpeedtestData{ +type +year +quarter +set_speedtest_data() +tile_prep() } class FacebookData{ +locations +access_token +ad_account_id +call_limit +radius +set_fb_data() } class OpenCellData{ +access_token +set_cell_data() +call_prep() }","title":"OpenData:"},{"location":"datagat/#country","text":"classDiagram Country < |-- School Country < |-- Survey Survey < |-- BRA_Survey Survey < |-- THA_Survey class Country{ +set_country_geometry() } class School{ +buffer +set_school_data() +school_prep() } class BRA_Survey{ +get_area_links() +set_survey_data() } class THA_Survey{ +set_area_data() +set_survey_data() }","title":"Country:"},{"location":"datagat/#data-dictionary","text":"Show a table of each of the predictors and what their definitions are: Variable Name Description Data Source avg_d_kbp3 Average Download Speed Speedtest Data avg_u_kbps Average Upload Speed Speedtest Data estimate_dau Facebook Daily Active Users estimate Facebook API estimate_mau Facebook Monthly Active Users estimate Facebook API population Population within a 1 km buffer zone, estimated with ?? Population Data pop_norm Population normalized Population Data mean_ghm Mean Global Human Modification value Global Human Modification Index mean_avg_rad Mean value from the Average Radiance band VIIRS Nighttime DNB mean_cf_cvg Mean value from the cloud free coverage band VIIRS Nighttime DNB slope_year_avg_rad The yearly rate of change between 2019 and 2014 of Average Radiance VIIRS Nighttime DNB change_year_avg_rad The change between the average values of 2019 and 2014 of Average Radiance VIIRS Nighttime DNB slope_year_cf_cvg The yearly rate of change between 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB change_year_cf_cvg The change between the average values of 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB slope_month_avg_rad The monthly rate of change between 2019 and 2014 of the Average Radiance Band VIIRS Nighttime DNB change_month_avg_rad The change between the average of Dec 2019 and Jan 2014 of the Average Radiance Band VIIRS Nighttime DNB slope_month_cf_cvg The monthly rate of change between 2019 and 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB change_month_cf_cvg The rate of change between the average of Dec 2019 and Jan 2014 from the Cloud Free Coverage Band VIIRS Nighttime DNB mean_NDVI The average value of the Vegetation Index MODIS Dataset slope_year_NDVI The yearly rate of change between 2019 and 2014 of the Vegetation Index MODIS Dataset change_year_NDVI The change between 2019 and 2014 of the Vegetation Index MODIS Dataset slope_month_NDVI The monthly rate of change between 2019 and 2014 of the Vegetation Index MODIS Dataset change_month_NDVI The change between the average of May 2019 and May 2014 of the Vegetation Index MODIS Dataset","title":"Data Dictionary"},{"location":"eda/","text":"The first thing we wanted to explore in our Exploratory Data Analysis was some maps of what our countries looked like and how our predictors might map onto our countries. We used Google Earth Engine to create some maps of nighttime imagery, the global human modification index and the vegetation index. For nighttime and vegetation index, we also wanted to show the change in time as we were using the rate of change as a predictor as well. Below you will find some static images of the maps we created. If you click on them, you can also find an interactive version. Click here to see the Jupyter notebook with code included for replicating the maps below and here for the html version . Satellite Images on a National Level for both Brazil and Thailand: Average Radiance Band Here we see that the Average light comes from the big cities in the south for both countries. This predictor later plays a big role in determining internet connectivity. Click on this map to see a comparison between school points and the entire country average radiance in 2014 and in 2019. Cloud Free Band This is a second band within the VIIRS Satellite nighttime images. It measures light without clouds or solar illumination. In some ways, specifically in tropical rainforests which both Brazil and Thailand have, it is a better measure of light emittance than the average radiance band. We use both as predictors in our model. Additionally, you see in the maps that the light emittance looks vastly different. Click on this map to see a comparison between school points and the entire country cloud free coverage in 2014 and in 2019. Global Human Modification Map In this map, we see the level of Global Human Modification in the last few years within both Brazil and Thailand. For more information on how this dataset was compiled, please see the Data Gathering page. Here you can also examine the entire country: Brazil Map Normalized Difference Vegetation Index Here we see the difference in vegetation between Brazil and Thailand. Click here to see the map for Brazil, toggle between the layers to see the entire country and just the school point areas. Map depicting NDVI change Here we also see a GIF that shows the time series change of Vegetation from 2000 to 2021. Speedtest data Cell data Facebook Data? We also did some Exploratory Data Analysis once our training dataset was created. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file.","title":"Exploratory Data Analysis"},{"location":"fe/","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Feature Engineering Having retrieved data of many different types, at different geospatial resolutions, from an array of different sources, it was necessary to develop a robust and comprehensive feature engineering pipeline, which produces clean datasets, ready for model training or application. Data Cleaning Numerical Variables - Impute missing values as variable median. Categorical Variables - Fill missing values with 'missing' label; perform one-hot encoding. Target Variable Our target variable is ground-truth survey data on local internet connectivity. For the Brazilian and Thai surveys, fowarded to us by ITU, these target variables correspond to the following labels: Brazil - A4A Thailand - H107 Joining Locations In the following description, we use the word 'feature' to mean predictor and, in some cases, ground truth survey variable. The map_feature method within the FeatureEngineering class is used to perform a spatial join between school locations and feature values. In essence, we are finding the correct value of each feature at each given school location. To do so, we first ensure that the dataframe corresponding to each predictor or survey dataset is loaded as a geodataframe, with an appropriately defined 'geometry' column. If the feature's geometry is a polygon, or multipolygon, rather than a point, we take the centroid as the location with which to match. If, instead, the feature's geometry is defined by latitude and longitude columns, rather than a single geometry column, this will be handled appropriately, so long as these column names are exactly 'latitude' and 'longitude', or 'lat' and 'lon'. For the unexpected instance in which schools have already been matched to feature values, we look for a 'source_school_id' column and merge the school data to the feature data according to these school ids. This is for the sake of code robustness, in case features that are already mapped to schools are mistakenly passed to the map_feature method. The build_tree method is then called to implement Scikit-learn's KDTree package and thus build a kd tree of the previously defined centroids. The kd tree is a tree in k-dimensional space; for us, this defines the spatial relationships between locations. This tree is then queried with the school locations to get the nearest neighbours, with the query returning dist (geographic distance) and ind (index associated with this location). The rows in the school_data dataframe are then assigned the correct values for each new column. For any variable named 'range', such as the mobile cell tower range variable from the OpenCellID dataset, values are converted to a 0 or 1, corresponding to range < dist and range >= dist respectively. This last part converts any 'range' to a binary variable, representing out-of-range, or in-range. If Dataset Is Sparsely Scattered across Country: It should be noted that the map_feature method should only be used for features that are not sparsely distributed. For example, this method is not used for the Brazil survey data, which is only available for a selection of enumeration areas, which often have significant geographic regions between them. For other features, such as the Brazil survey data, which have values only in sparsely distributed locations, we employ the map_enumeration method, which joins school locations to areas via intersections between the enumeration area polygons and 1km radius school buffer zones. We then check for instances in which a school might have been joined to multiple enumeration areas and select only the nearest enumeration area for such cases. High Level Feature Engineering Pipeline graph TD A[Get School Data] --> B{Is Survey Available?}; B --> |Yes| C[Get Survey Data]; B --> |No| D[Load Predictor Dataset]; C --> E D --> E[Initialise New Columns]; E --> F[Clean New Data]; F --> G[Match New Data to Schools]; G --> H{All Features Added?}; H --> |Yes| I[Save Dataset] H --> |No| D;","title":"Feature Engineering"},{"location":"fe/#feature-engineering","text":"Having retrieved data of many different types, at different geospatial resolutions, from an array of different sources, it was necessary to develop a robust and comprehensive feature engineering pipeline, which produces clean datasets, ready for model training or application.","title":"Feature Engineering"},{"location":"fe/#data-cleaning","text":"Numerical Variables - Impute missing values as variable median. Categorical Variables - Fill missing values with 'missing' label; perform one-hot encoding.","title":"Data Cleaning"},{"location":"fe/#target-variable","text":"Our target variable is ground-truth survey data on local internet connectivity. For the Brazilian and Thai surveys, fowarded to us by ITU, these target variables correspond to the following labels: Brazil - A4A Thailand - H107","title":"Target Variable"},{"location":"fe/#joining-locations","text":"In the following description, we use the word 'feature' to mean predictor and, in some cases, ground truth survey variable. The map_feature method within the FeatureEngineering class is used to perform a spatial join between school locations and feature values. In essence, we are finding the correct value of each feature at each given school location. To do so, we first ensure that the dataframe corresponding to each predictor or survey dataset is loaded as a geodataframe, with an appropriately defined 'geometry' column. If the feature's geometry is a polygon, or multipolygon, rather than a point, we take the centroid as the location with which to match. If, instead, the feature's geometry is defined by latitude and longitude columns, rather than a single geometry column, this will be handled appropriately, so long as these column names are exactly 'latitude' and 'longitude', or 'lat' and 'lon'. For the unexpected instance in which schools have already been matched to feature values, we look for a 'source_school_id' column and merge the school data to the feature data according to these school ids. This is for the sake of code robustness, in case features that are already mapped to schools are mistakenly passed to the map_feature method. The build_tree method is then called to implement Scikit-learn's KDTree package and thus build a kd tree of the previously defined centroids. The kd tree is a tree in k-dimensional space; for us, this defines the spatial relationships between locations. This tree is then queried with the school locations to get the nearest neighbours, with the query returning dist (geographic distance) and ind (index associated with this location). The rows in the school_data dataframe are then assigned the correct values for each new column. For any variable named 'range', such as the mobile cell tower range variable from the OpenCellID dataset, values are converted to a 0 or 1, corresponding to range < dist and range >= dist respectively. This last part converts any 'range' to a binary variable, representing out-of-range, or in-range.","title":"Joining Locations"},{"location":"fe/#if-dataset-is-sparsely-scattered-across-country","text":"It should be noted that the map_feature method should only be used for features that are not sparsely distributed. For example, this method is not used for the Brazil survey data, which is only available for a selection of enumeration areas, which often have significant geographic regions between them. For other features, such as the Brazil survey data, which have values only in sparsely distributed locations, we employ the map_enumeration method, which joins school locations to areas via intersections between the enumeration area polygons and 1km radius school buffer zones. We then check for instances in which a school might have been joined to multiple enumeration areas and select only the nearest enumeration area for such cases.","title":"If Dataset Is Sparsely Scattered across Country:"},{"location":"fe/#high-level-feature-engineering-pipeline","text":"graph TD A[Get School Data] --> B{Is Survey Available?}; B --> |Yes| C[Get Survey Data]; B --> |No| D[Load Predictor Dataset]; C --> E D --> E[Initialise New Columns]; E --> F[Clean New Data]; F --> G[Match New Data to Schools]; G --> H{All Features Added?}; H --> |Yes| I[Save Dataset] H --> |No| D;","title":"High Level Feature Engineering Pipeline"},{"location":"folder_structure/","text":"Folder Structure files/ src/ scripts/ configs.py main.py data_pipeline.py country.py opendata.py school.py survey.py opendata_facebook.py opendata_scrap.py data/ geodata/ school_loc/ enumeration_area/ fb/ opencellid/ satellite/ speedtest/ survey/ training_sets/ worldpop/","title":"Folder Structure"},{"location":"folder_structure/#folder-structure","text":"files/ src/ scripts/ configs.py main.py data_pipeline.py country.py opendata.py school.py survey.py opendata_facebook.py opendata_scrap.py data/ geodata/ school_loc/ enumeration_area/ fb/ opencellid/ satellite/ speedtest/ survey/ training_sets/ worldpop/","title":"Folder Structure"},{"location":"intro/","text":"Introduction What is the problem? Lots of countries around the world, and specific communities within those countries do not have access to the internet. In order to succeed in our fast paced world, it would be beneficial for them to have it. Thus our project seeks to accurately identify schools and communities lacking internet access in order to help Project Giga reach them efficiently. Motivation Connecting to the internet is extremely important in this day and age. By 2030, Project Giga hopes to have every school in the world to have an internet connection. Internet serves as a crucial background to local economies, economic development and individual's ability to get jobs and communicate with the outside world. Use Case We are trying to predict multiple objectives. Our first objective is to determine on a scale of 0 to 1, the proportion of internet users in a specific area of a country. We are using schools as the anchor points of these predictions as they are also community hubs. Therefore our predictions seek to determine the level of internet connectivity around a school in order to help Project Giga prioritize which schools to connect. Intenet Connectivity in this instance is defined as any ability to get online. This means both broadband and mobile internet connection. Additionally, it means that individuals must have devices to be able to connect to the internet. Our second objective, for ITU, is to aggregate our understanding of internet connectivity on a school level up to a country level. While this is not helpful for some countries where national level internet statistics already exist, it will be helpful in countries where there are not currently surveys or established statistics. Our model will establish a baseline metric for a country and build greater understanding of countries that have lower or high internet connectivity.","title":"Introduction"},{"location":"intro/#introduction","text":"","title":"Introduction"},{"location":"intro/#what-is-the-problem","text":"Lots of countries around the world, and specific communities within those countries do not have access to the internet. In order to succeed in our fast paced world, it would be beneficial for them to have it. Thus our project seeks to accurately identify schools and communities lacking internet access in order to help Project Giga reach them efficiently.","title":"What is the problem?"},{"location":"intro/#motivation","text":"Connecting to the internet is extremely important in this day and age. By 2030, Project Giga hopes to have every school in the world to have an internet connection. Internet serves as a crucial background to local economies, economic development and individual's ability to get jobs and communicate with the outside world.","title":"Motivation"},{"location":"intro/#use-case","text":"We are trying to predict multiple objectives. Our first objective is to determine on a scale of 0 to 1, the proportion of internet users in a specific area of a country. We are using schools as the anchor points of these predictions as they are also community hubs. Therefore our predictions seek to determine the level of internet connectivity around a school in order to help Project Giga prioritize which schools to connect. Intenet Connectivity in this instance is defined as any ability to get online. This means both broadband and mobile internet connection. Additionally, it means that individuals must have devices to be able to connect to the internet. Our second objective, for ITU, is to aggregate our understanding of internet connectivity on a school level up to a country level. While this is not helpful for some countries where national level internet statistics already exist, it will be helpful in countries where there are not currently surveys or established statistics. Our model will establish a baseline metric for a country and build greater understanding of countries that have lower or high internet connectivity.","title":"Use Case"},{"location":"modapp/","text":"Model Application Thailand Our next big step was applying the best model to Thailand data. We were curious to apply the model as we were not sure that the same assumptions that are true for Brazil would hold true for Thailand. While the satellite data and vegetation may look the same, the national level economic and political indicators were not accounted for in the model. This is because, due to the project scope and capacity, we did not train multiple different national models. Had we had more time and data, perhaps this would have been an alternative route and we could have included some of this information. Instead, we trained a model exclusively on Brazil. For more discussion on future multi-national models, please see the conclusion. Therefore, the limitations for our model rooted in basic assumptions that local areas can be comparable. Our second set of limitations was in the nature of the Thailand data. We wanted to predict and evaluate the Thai schools in the same manner that we did for the Brazil schools. However, the survey data that served as ground truth for Brazil was on an enumeration area level while the survey data for Thailand was on a province area level (of which there are 77 in Thailand). These area units are not comparable and therefore made the evaluation for Thailand more complicated. Below you can see our predictions on a school level which look generally good, though there is no ground truth by which to evaluate. We then scale these school predictions up to a province level. When we get to the province level evaluation, our predictions look much worse. This perhaps can reflect upon our model and its questionable performance, but it also reflects on the raw survey data itself as we are skeptical of the amount of provinces that have 100% internet connectivity to begin with. Steps in our model application to new data. Please click here for a complete predict.py script. Click here for a Jupyter notebook with the XGBoost Predictions and its html equivalent. Using the model_config, we load the Thailand data with the school points and the same predictors used by the original model. Then, we load the model from mlflow where it was pickled as an artifact. Here's some code showing how it was reloaded. Then we examine the predictions on a map: Here are the maps that show the schools' predictions from 0-1 in Thailand. These are all the schools in Thailand, as one can tell it looks reasonable. Here are the schoools just below 50% internet connectivity, predicted by the best Random Forest Model and the best XGBoost Model. There are 97 schools predicted in both, but slightly different pattern of schools. In order to compare our predictions to the ground truth, we aggregated the schools up to a province level as we only have the survey data on that level. This proved challenging for a number of reasons as stated above. Here is what our model prediction look like compared to ground truth, as you can tell they are very different from each other: While our mean province level error is .35, which is not terrible, we can see that the model predictions on a province level diverge greatly from the existing ground truth. Therefore, we are uncertain about the ability for our Brazil model to accurately predict schools with low internet connectivity in Thailand. Philippines We also were able to test this out on the Philippines. The Philippines had better data as their surveys were on an enumeration area level. Here are the results from our Philippines predictions.","title":"Model Application"},{"location":"modapp/#model-application","text":"","title":"Model Application"},{"location":"modapp/#thailand","text":"Our next big step was applying the best model to Thailand data. We were curious to apply the model as we were not sure that the same assumptions that are true for Brazil would hold true for Thailand. While the satellite data and vegetation may look the same, the national level economic and political indicators were not accounted for in the model. This is because, due to the project scope and capacity, we did not train multiple different national models. Had we had more time and data, perhaps this would have been an alternative route and we could have included some of this information. Instead, we trained a model exclusively on Brazil. For more discussion on future multi-national models, please see the conclusion. Therefore, the limitations for our model rooted in basic assumptions that local areas can be comparable. Our second set of limitations was in the nature of the Thailand data. We wanted to predict and evaluate the Thai schools in the same manner that we did for the Brazil schools. However, the survey data that served as ground truth for Brazil was on an enumeration area level while the survey data for Thailand was on a province area level (of which there are 77 in Thailand). These area units are not comparable and therefore made the evaluation for Thailand more complicated. Below you can see our predictions on a school level which look generally good, though there is no ground truth by which to evaluate. We then scale these school predictions up to a province level. When we get to the province level evaluation, our predictions look much worse. This perhaps can reflect upon our model and its questionable performance, but it also reflects on the raw survey data itself as we are skeptical of the amount of provinces that have 100% internet connectivity to begin with. Steps in our model application to new data. Please click here for a complete predict.py script. Click here for a Jupyter notebook with the XGBoost Predictions and its html equivalent. Using the model_config, we load the Thailand data with the school points and the same predictors used by the original model. Then, we load the model from mlflow where it was pickled as an artifact. Here's some code showing how it was reloaded. Then we examine the predictions on a map: Here are the maps that show the schools' predictions from 0-1 in Thailand. These are all the schools in Thailand, as one can tell it looks reasonable. Here are the schoools just below 50% internet connectivity, predicted by the best Random Forest Model and the best XGBoost Model. There are 97 schools predicted in both, but slightly different pattern of schools. In order to compare our predictions to the ground truth, we aggregated the schools up to a province level as we only have the survey data on that level. This proved challenging for a number of reasons as stated above. Here is what our model prediction look like compared to ground truth, as you can tell they are very different from each other: While our mean province level error is .35, which is not terrible, we can see that the model predictions on a province level diverge greatly from the existing ground truth. Therefore, we are uncertain about the ability for our Brazil model to accurately predict schools with low internet connectivity in Thailand.","title":"Thailand"},{"location":"modapp/#philippines","text":"We also were able to test this out on the Philippines. The Philippines had better data as their surveys were on an enumeration area level. Here are the results from our Philippines predictions.","title":"Philippines"},{"location":"model/","text":"Modeling Section: The model we need to train is a regression model as we are attempting to predict a number between 0 and 1 of internet connectivity. A result or prediction of 0 means that of the households surveyed (about 11), no households in the enumeration area stated that they had access to internet. A result or prediction of 1 means that every household surveyed in the enumeration area had access to internet. Most responses fell on a scale between 0 and 1, indicating that some but not all families had internet access. Later on, we attempted to turn this into a classification problem to check our work but it did not provide any higher accuracy. Training Set EDA We also did some Exploratory Data Analysis once our training dataset was created. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file. Mlflow Set-up (Optional) In order to track our models, we set up autologging in mlflow. Mlflow is an exciting and experimental way of logging models. We set up our model training so that our python script for each model would create a new experiment for each run, it would log each of our model parameters when we did hyperparameter tuning and then log the best parameter at the top. In this way we were able to compare the various parameters logged in each run to determine how to change the grid space of the hyperparameters. We also were then able to compare models to each other. Within mlflow, we also logged the predictors for each run and the requirements for packages and dependencies to run. Each run also logs the best model as an artifact, so one can easily take the model and apply it to new data. We are including both the best model logged, as well as each run, here in order to make this as reproducible as possible. Below you can see a screenshot of a mlflow which logs our best runs, with our best hyperparameters and using our custom metric for evaluation. On the side, you can also see the list of other experiments we ran with different models. code snippet for input and output? Here you can see the simplicity of reload the model artifact later on and applying on new data: Model Configuration Once we have mlflow set up and our model_config.yaml file set up, we can run many different experiments using our .py scripts by changing a few things within our yaml file. Click here (INSERT YAML Script) to see the full yaml file. Below you can also see how it was set up. We use this to steer our scripts and set our parameters. We set up the input data at the top which is our training data, then label the target and predictor variables as well as the name of the experiment and a brief description in run_name. Under the parameters section, we set parameters like test size (which is crucial), the amount of cross validation folds to do, the number of iterations and the threshold for our custom metric. The threshold tells the model which percent of schools with low internet connectivity to focus on. Then within parameters, there are different sections based on what type of model you might decide to run. Our .yaml file contains parameters for grid search within Random Forest, LightGBM and XGBoost. Model Training We tried out 7 different model classes and ran over 100 experiments each containing 20 or more runs that tried various parameters in order to determine which model had the best accuracy. We experimented with various parameters, as well as different combinations of predictors. Below is the final list of predictors we used and a heat map displaying their collinearity. As you can see, there is not high multi-collinearity among our predictors except with the mean global human modification and the mean average radiance. However, we felt both predictors were important and had high feature importance in the model so we decided to keep both in. In the second figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance have strong correlation. In this figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance have strong correlation. Another way that we improved accuracy was by building a custom metric in order to score both our test set within our cross validation and our final holdout set. The metric calculates errors specifically by taking the prediction below .3 (or another threshold, we also experimented with .5) subtracting that from the ground truth below .3 (or another threshold), taking the absolute value and then returning the average of all those errors. Below please find a code snippet of our custom metric. def custom metric We built this as we understood that it was more important to have better accuracy on schools with lower internet connectivity than higher. Before insitituting the custom metric, our models were good with predicting the average values, but they did poorly at either end of the spectrum and particularly on the low values. In order to remedy this issue, we first dropped any rows that had an internet connectivity of zero (there were 23 of them). We dropped the zero's because our project partners informed us that they were most likely due to incomplete data and because they skewed our results. Because there were only 23 of them, we felt it did not impact the data class balancing. Secondly, we instituted our custom metric which trained the model to minimize the error score under the .3 level of prediction. The resulting champion out of over 2000 models was XGBoost with an average error of .06 and specifically for under the .3 threshold, had an average error of .05. This means that for schools that are predicted to be below 30%, we can trust the model's predictions, as on average the predictions are only off by 5 percentage points from the ground truth value. Below, you can see the list of all the model classes we tried. Feel free to try out running these models yourselves or reading the code by clicking on the hyper linked script. There is further documentation within each script on how it runs, and how it works with mlflow logging. Linear Regression (see train_Linear_Regression.py) - Python script with Mlflow #is this right? Random Forest HTML File Jupyter Notebook Python Script with Mlflow Python script without Mlflow XGBoost (See train_XGBoost_Exp1.py file with mlflow logging and without) HTML File Jupyter Notebook #this is not correct Python Script with Mlflow Python script without Mlflow LightGBM (See Jupyter Notebook for full training and evaluation of errors)(see also lightgbm_mlflow_train.py) HTML File Jupyter Notebook Python Script with Mlflow SVM Python Script with Mlflow Neural Net Python Script with Mlflow Random Forest Classifier (See Jupyter Notebook for training, was used as a check on other models) HTML File Jupyter Notebook Here we see a comparison of all the models. It is clear that Random Forest and XGBoost both have the lowest average error among all the models, therefore they are the winners. Click on this link to see a notebook with the model comparisons (Also save as HTML to embed into documentation) Model Evaluation and Results As you can see from the above graph, our winning model was the XGboost model which produced an error of .06 and a low average error of .05 with the hyper parameters of: eta: .2, max_depth: 9, n_estimators: 550. Click on this link for the notebook with the Random Forest Predictions and click on this link for the notebook with the XGBoost Predictions. Here is a map of our predictions for schools within Brazil. Figure 1 displays the location for all the schools were the ground truth is less than 30% connected to the internet. There are 69 schools in Brazil that have less than 30% internet connectivity. Figure 2 shows the errors in schools where the prediction is less than 30% connected to the internet. While we can see that there are fewer schools that are predicted than that exist, we can trust that our predictions are correct, as the error score is low. This map was made using the Random Forest model which predicts 14 schools. The XGBoost model predicts 29 schools below 30%. Additionally, our predicted schools match up with our ground truth schools. In Figure 3, we see the predictions for all the schools in the test set mapped out. This gives us an understanding of where the higher and lower connected schools are located regionally. It appears that the higher connected schools are on the coast (the yellows and light greens) while the lower connected schools are located more inland. In Figure 4, we see the errors mapped out for the schools in the test set. As we can see most schools have a low error score, which means we can mostly trust the predictions. The schools with higher error scores are also the schools that have less connectivity, which provides even more motivation to use our custom metric as we want to focus on having a lower error score for schools that are less connected. Thus the 14 schools depicted in Figure 2 are the ones that one could prioritize to connect. We also see that our predictions closely mirror the ground truth within the country, as well as an external data source titled Digital 2021: Brazil. Thus we can trust that our model performs well on Brazilian school data. Then we see the residuals compared to reality. This is promising as most residuals hug tightly to the line except for the ones at the very low and high end. Lastly, we also see the comparison of distributions between reality and predictions. While the predictions are a bit higher, the overall curves generally follow each other. Ask Jacob about prediction 1 vs 2 ? Lastly, we can see our residuals plotted again the ground truth. In the first image, we see the Random Forest model predictions against the ground truth, for the most part the predictions are close with the ground truth except in the lower threshold area. Model Interpretation As part of our winning models, we wanted to see which predictors had high feature importance within the model. Below, is the graph for both Random Forest and XGBoost feature importances. As one can see, the highest feature importances are the nighttime average radiance predictor and the Facebook monthly active users. Here are the examinations of the shapely values for feature importances. @Jacob or Utku to put in pics","title":"Modeling"},{"location":"model/#modeling-section","text":"The model we need to train is a regression model as we are attempting to predict a number between 0 and 1 of internet connectivity. A result or prediction of 0 means that of the households surveyed (about 11), no households in the enumeration area stated that they had access to internet. A result or prediction of 1 means that every household surveyed in the enumeration area had access to internet. Most responses fell on a scale between 0 and 1, indicating that some but not all families had internet access. Later on, we attempted to turn this into a classification problem to check our work but it did not provide any higher accuracy.","title":"Modeling Section:"},{"location":"model/#training-set-eda","text":"We also did some Exploratory Data Analysis once our training dataset was created. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file.","title":"Training Set EDA"},{"location":"model/#mlflow-set-up-optional","text":"In order to track our models, we set up autologging in mlflow. Mlflow is an exciting and experimental way of logging models. We set up our model training so that our python script for each model would create a new experiment for each run, it would log each of our model parameters when we did hyperparameter tuning and then log the best parameter at the top. In this way we were able to compare the various parameters logged in each run to determine how to change the grid space of the hyperparameters. We also were then able to compare models to each other. Within mlflow, we also logged the predictors for each run and the requirements for packages and dependencies to run. Each run also logs the best model as an artifact, so one can easily take the model and apply it to new data. We are including both the best model logged, as well as each run, here in order to make this as reproducible as possible. Below you can see a screenshot of a mlflow which logs our best runs, with our best hyperparameters and using our custom metric for evaluation. On the side, you can also see the list of other experiments we ran with different models. code snippet for input and output? Here you can see the simplicity of reload the model artifact later on and applying on new data:","title":"Mlflow Set-up (Optional)"},{"location":"model/#model-configuration","text":"Once we have mlflow set up and our model_config.yaml file set up, we can run many different experiments using our .py scripts by changing a few things within our yaml file. Click here (INSERT YAML Script) to see the full yaml file. Below you can also see how it was set up. We use this to steer our scripts and set our parameters. We set up the input data at the top which is our training data, then label the target and predictor variables as well as the name of the experiment and a brief description in run_name. Under the parameters section, we set parameters like test size (which is crucial), the amount of cross validation folds to do, the number of iterations and the threshold for our custom metric. The threshold tells the model which percent of schools with low internet connectivity to focus on. Then within parameters, there are different sections based on what type of model you might decide to run. Our .yaml file contains parameters for grid search within Random Forest, LightGBM and XGBoost.","title":"Model Configuration"},{"location":"model/#model-training","text":"We tried out 7 different model classes and ran over 100 experiments each containing 20 or more runs that tried various parameters in order to determine which model had the best accuracy. We experimented with various parameters, as well as different combinations of predictors. Below is the final list of predictors we used and a heat map displaying their collinearity. As you can see, there is not high multi-collinearity among our predictors except with the mean global human modification and the mean average radiance. However, we felt both predictors were important and had high feature importance in the model so we decided to keep both in. In the second figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance have strong correlation. In this figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance have strong correlation. Another way that we improved accuracy was by building a custom metric in order to score both our test set within our cross validation and our final holdout set. The metric calculates errors specifically by taking the prediction below .3 (or another threshold, we also experimented with .5) subtracting that from the ground truth below .3 (or another threshold), taking the absolute value and then returning the average of all those errors. Below please find a code snippet of our custom metric. def custom metric We built this as we understood that it was more important to have better accuracy on schools with lower internet connectivity than higher. Before insitituting the custom metric, our models were good with predicting the average values, but they did poorly at either end of the spectrum and particularly on the low values. In order to remedy this issue, we first dropped any rows that had an internet connectivity of zero (there were 23 of them). We dropped the zero's because our project partners informed us that they were most likely due to incomplete data and because they skewed our results. Because there were only 23 of them, we felt it did not impact the data class balancing. Secondly, we instituted our custom metric which trained the model to minimize the error score under the .3 level of prediction. The resulting champion out of over 2000 models was XGBoost with an average error of .06 and specifically for under the .3 threshold, had an average error of .05. This means that for schools that are predicted to be below 30%, we can trust the model's predictions, as on average the predictions are only off by 5 percentage points from the ground truth value. Below, you can see the list of all the model classes we tried. Feel free to try out running these models yourselves or reading the code by clicking on the hyper linked script. There is further documentation within each script on how it runs, and how it works with mlflow logging. Linear Regression (see train_Linear_Regression.py) - Python script with Mlflow #is this right? Random Forest HTML File Jupyter Notebook Python Script with Mlflow Python script without Mlflow XGBoost (See train_XGBoost_Exp1.py file with mlflow logging and without) HTML File Jupyter Notebook #this is not correct Python Script with Mlflow Python script without Mlflow LightGBM (See Jupyter Notebook for full training and evaluation of errors)(see also lightgbm_mlflow_train.py) HTML File Jupyter Notebook Python Script with Mlflow SVM Python Script with Mlflow Neural Net Python Script with Mlflow Random Forest Classifier (See Jupyter Notebook for training, was used as a check on other models) HTML File Jupyter Notebook Here we see a comparison of all the models. It is clear that Random Forest and XGBoost both have the lowest average error among all the models, therefore they are the winners. Click on this link to see a notebook with the model comparisons (Also save as HTML to embed into documentation)","title":"Model Training"},{"location":"model/#model-evaluation-and-results","text":"As you can see from the above graph, our winning model was the XGboost model which produced an error of .06 and a low average error of .05 with the hyper parameters of: eta: .2, max_depth: 9, n_estimators: 550. Click on this link for the notebook with the Random Forest Predictions and click on this link for the notebook with the XGBoost Predictions. Here is a map of our predictions for schools within Brazil. Figure 1 displays the location for all the schools were the ground truth is less than 30% connected to the internet. There are 69 schools in Brazil that have less than 30% internet connectivity. Figure 2 shows the errors in schools where the prediction is less than 30% connected to the internet. While we can see that there are fewer schools that are predicted than that exist, we can trust that our predictions are correct, as the error score is low. This map was made using the Random Forest model which predicts 14 schools. The XGBoost model predicts 29 schools below 30%. Additionally, our predicted schools match up with our ground truth schools. In Figure 3, we see the predictions for all the schools in the test set mapped out. This gives us an understanding of where the higher and lower connected schools are located regionally. It appears that the higher connected schools are on the coast (the yellows and light greens) while the lower connected schools are located more inland. In Figure 4, we see the errors mapped out for the schools in the test set. As we can see most schools have a low error score, which means we can mostly trust the predictions. The schools with higher error scores are also the schools that have less connectivity, which provides even more motivation to use our custom metric as we want to focus on having a lower error score for schools that are less connected. Thus the 14 schools depicted in Figure 2 are the ones that one could prioritize to connect. We also see that our predictions closely mirror the ground truth within the country, as well as an external data source titled Digital 2021: Brazil. Thus we can trust that our model performs well on Brazilian school data. Then we see the residuals compared to reality. This is promising as most residuals hug tightly to the line except for the ones at the very low and high end. Lastly, we also see the comparison of distributions between reality and predictions. While the predictions are a bit higher, the overall curves generally follow each other. Ask Jacob about prediction 1 vs 2 ? Lastly, we can see our residuals plotted again the ground truth. In the first image, we see the Random Forest model predictions against the ground truth, for the most part the predictions are close with the ground truth except in the lower threshold area.","title":"Model Evaluation and Results"},{"location":"model/#model-interpretation","text":"As part of our winning models, we wanted to see which predictors had high feature importance within the model. Below, is the graph for both Random Forest and XGBoost feature importances. As one can see, the highest feature importances are the nighttime average radiance predictor and the Facebook monthly active users. Here are the examinations of the shapely values for feature importances. @Jacob or Utku to put in pics","title":"Model Interpretation"},{"location":"Images/Images/","text":"","title":"Images"}]}